{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as dtparse\n",
    "import requests\n",
    "\n",
    "from WikiComponents import Article, WikiAPI, WikiBrowser\n",
    "from helper import read_csv, calc_freq, is_bot_user\n",
    "\n",
    "\n",
    "def get_first_query_page(resp: dict) -> dict:\n",
    "    pg = resp['query']['pages']\n",
    "    if len(pg) != 1:\n",
    "        print(\"[W] len(pg) != 1\")\n",
    "    k = list(pg.keys())[0]\n",
    "    return pg[k]\n",
    "\n",
    "\n",
    "def get_contributors(article_id):\n",
    "    # todo Use combined query makes more efficient.\n",
    "    _url = \"api.php?action=query&titles={article_id}&prop=contributors\"\n",
    "    resp = WikiAPI.get(_url.format(article_id=article_id))\n",
    "    first = get_first_query_page(resp)\n",
    "    n_contributers_a = first['anoncontributors']\n",
    "    n_contributers = len(first['contributors'])\n",
    "    return {'anoncontributors': n_contributers_a, 'contributors': n_contributers}\n",
    "\n",
    "\n",
    "def get_revision(article_id):\n",
    "    _url = 'api.php?action=query&prop=revisions&titles={article_id}' \\\n",
    "           '&rvprop=timestamp|user|userid&rvlimit=max'\n",
    "    u = _url.format(article_id=article_id)\n",
    "\n",
    "    revs = {}\n",
    "\n",
    "    def proc(resp: dict):\n",
    "        \"\"\"\n",
    "        \"revisions\": [\n",
    "        {\n",
    "            \"user\": \"74.96.187.144\",\n",
    "            \"anon\": \"\",\n",
    "            \"userid\": 0,\n",
    "            \"timestamp\": \"2009-11-22T17:04:09Z\"\n",
    "        },\n",
    "        {\n",
    "            \"user\": \"Miym\",\n",
    "            \"userid\": 8436643,\n",
    "            \"timestamp\": \"2009-07-30T15:35:49Z\"\n",
    "        }\n",
    "        \"\"\"\n",
    "        first = get_first_query_page(resp)\n",
    "        for rv in first['revisions']:\n",
    "            uid = rv['userid']\n",
    "            if uid in revs:\n",
    "                revs[uid].append(rv)\n",
    "            else:\n",
    "                revs[uid] = [rv]\n",
    "\n",
    "    resp = WikiAPI.get(u)\n",
    "    proc(resp)\n",
    "\n",
    "    while resp.get('continue') and resp['continue'].get('rvcontinue'):\n",
    "        resp = WikiAPI.get(u, {'rvcontinue': resp['continue']['rvcontinue']})\n",
    "        proc(resp)\n",
    "\n",
    "    return revs\n",
    "\n",
    "\n",
    "def analysis_revision_info(dict_rev_info: dict):\n",
    "    # Number of edits per editors\n",
    "    pass\n",
    "\n",
    "    # Frequency of edits (time between edits)\n",
    "    overall_edit_timestamp = []\n",
    "    overall_edit_nonbot_timestamp = []\n",
    "    for editor in dict_rev_info.values():\n",
    "        is_bot = is_bot_user(editor[0]['user'], editor[0]['userid'])\n",
    "        for rv in editor:\n",
    "            ts = rv.get('timestamp')\n",
    "            ts = dtparse(ts).timestamp()\n",
    "            rv['timestamp_parsed'] = ts\n",
    "            overall_edit_timestamp.append(ts)\n",
    "            if not is_bot:\n",
    "                overall_edit_nonbot_timestamp.append(ts)\n",
    "\n",
    "    freq_overall = calc_freq(overall_edit_timestamp)\n",
    "\n",
    "    # edit freq per editor\n",
    "    for uid, revisions in dict_rev_info.items():\n",
    "        dict_rev_info['edit_freq'] = calc_freq([rv['timestamp_parsed'] for rv in revisions])\n",
    "\n",
    "\n",
    "def count_talk_posts(article_id):\n",
    "    html_root = WikiBrowser.get_talk_page(article_id)\n",
    "    return len(html_root.find('div', id='bodyContent').find_all('h2'))\n",
    "\n",
    "\n",
    "def num_editor_talk(article_id):\n",
    "    # Use an API provided by XTools Wiki-Project\n",
    "    _url = 'https://xtools.wmflabs.org/api/page/articleinfo/en.wikipedia.org/Talk:{article_id}'\n",
    "    resp = requests.get(_url.format(article_id=article_id)).json()\n",
    "    return resp['editors']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list = read_csv(\"data/articles.csv\",\n",
    "                        lambda row: Article(id_=row[1], grade=row[2]))\n",
    "\n",
    "# Group those `good` and `not-so-good` articles\n",
    "groupGood = list(filter(lambda i: i.grade in ['A', 'FA', 'GA'], article_list))\n",
    "groupNSGood = list(filter(lambda i: i.grade in ['C', 'Start'], article_list))\n",
    "\n",
    "print('groupGood: n =', len(groupGood))\n",
    "print('groupNSGood: n =', len(groupNSGood))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article = groupGood[0]\n",
    "\n",
    "# get unique contributor\n",
    "uniq_con = get_contributors(article.id_)\n",
    "print('unique contributor:', uniq_con)\n",
    "\n",
    "# get user revisions\n",
    "rvs = get_revision(article.id_)\n",
    "analysis_revision_info(rvs)\n",
    "print()\n",
    "\n",
    "# talk-page posts\n",
    "ctp = count_talk_posts(article.id_)\n",
    "print('talk-page posts n=', ctp)\n",
    "\n",
    "# Number of editors posting on talk-pages\n",
    "etp = num_editor_talk(article.id_)\n",
    "print('talk-page editor n=', etp)"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
